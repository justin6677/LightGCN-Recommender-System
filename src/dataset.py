# src/dataset.py (ä¿®æ­£ç‰ˆ)
import os
import numpy as np
import scipy.sparse as sp
from scipy.sparse import csr_matrix
import torch
from .utils import cprint

class Loader:
    """ Gowalla dataset Loader """
    # é€™è£¡ä¿®æ­£äº†ï¼šå¢åŠ  config, max_users, max_items åƒæ•¸
    def __init__(self, config, path="./data/gowalla", max_users=None, max_items=None):
        cprint(f'loading [{path}]')
        
        # æ¥æ”¶å‚³å…¥çš„ config
        self.split = config['A_split']
        self.folds = config['A_n_fold']
        self.path = path
        
        train_file = os.path.join(path, 'train.txt')
        test_file  = os.path.join(path, 'test.txt')

        trainUser, trainItem = [], []
        testUser,  testItem  = [], []

        self.traindataSize = 0
        self.testDataSize  = 0
        self.n_user = 0
        self.m_item = 0

        # ---- è®€ train.txt ----
        with open(train_file) as f:
            for l in f.readlines():
                if len(l) == 0: continue
                l = l.strip().split(' ')
                uid = int(l[0])
                
                # å¦‚æœæœ‰è¨­å®š max_usersï¼Œè¶…éå°±è·³é
                if max_users is not None and uid >= max_users:
                    continue
                    
                items = [int(i) for i in l[1:]]
                # å¦‚æœæœ‰è¨­å®š max_itemsï¼Œè¶…éå°±éæ¿¾
                if max_items is not None:
                    items = [i for i in items if i < max_items]
                
                if not items: continue
                
                trainUser.extend([uid] * len(items))
                trainItem.extend(items)
                self.m_item = max(self.m_item, max(items))
                self.n_user = max(self.n_user, uid)
                self.traindataSize += len(items)

        self.trainUser = np.array(trainUser)
        self.trainItem = np.array(trainItem)

        # ---- è®€ test.txt ----
        with open(test_file) as f:
            for l in f.readlines():
                if len(l) == 0: continue
                l = l.strip().split(' ')
                uid = int(l[0])
                
                if max_users is not None and uid >= max_users:
                    continue
                
                items = [int(i) for i in l[1:]]
                if max_items is not None:
                    items = [i for i in items if i < max_items]
                    
                if not items: continue
                
                testUser.extend([uid] * len(items))
                testItem.extend(items)
                self.m_item = max(self.m_item, max(items))
                self.n_user = max(self.n_user, uid)
                self.testDataSize += len(items)

        self.m_item += 1
        self.n_user += 1
        self.testUser = np.array(testUser)
        self.testItem = np.array(testItem)

        cprint(f"âœ… {self.traindataSize} train, {self.testDataSize} test interactions, "
               f"{self.n_user} users, {self.m_item} items")

        # ---- å»ºä½¿ç”¨è€…-ç‰©å“ç¨€ç–çŸ©é™£ ----
        self.UserItemNet = csr_matrix(
            (np.ones(len(self.trainUser)), (self.trainUser, self.trainItem)),
            shape=(self.n_user, self.m_item),
        )

        self._allPos = self.getUserPosItems(list(range(self.n_user)))
        self.__testDict = self.__build_test()
        self.Graph = None

    @property
    def n_users(self): return self.n_user
    @property
    def m_items(self): return self.m_item
    @property
    def trainDataSize(self): return self.traindataSize
    @property
    def testDict(self): return self.__testDict
    @property
    def allPos(self): return self._allPos

    def __build_test(self):
        test_data = {}
        for u, i in zip(self.testUser, self.testItem):
            if u in test_data: test_data[u].append(i)
            else: test_data[u] = [i]
        return test_data

    def getUserPosItems(self, users):
        posItems = []
        for user in users:
            posItems.append(self.UserItemNet[user].nonzero()[1])
        return posItems

    def _convert_sp_mat_to_sp_tensor(self, X):
        coo = X.tocoo().astype(np.float32)
        row = torch.LongTensor(coo.row)
        col = torch.LongTensor(coo.col)
        index = torch.stack([row, col])
        data = torch.FloatTensor(coo.data)
        return torch.sparse.FloatTensor(index, data, torch.Size(coo.shape))

# è«‹æŠŠåŸæœ¬çš„ getSparseGraph åˆªæ‰ï¼Œæ›æˆé€™å€‹å„ªåŒ–ç‰ˆ
    def getSparseGraph(self):
        # è‡ªå‹•åµæ¸¬ device
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        if self.Graph is not None:
            return self.Graph

        cprint("generating adjacency matrix")
        
        n_users, m_items = self.n_user, self.m_item
        
        # ==========================================
        # ğŸ’¡ è¨˜æ†¶é«”å„ªåŒ–é‡é» (Memory Optimization)
        # ä¸è¦ç”¨ adj_mat[slice] = R çš„æ–¹å¼ï¼Œå› ç‚ºæœƒè§¸ç™¼è¨˜æ†¶é«”è¤‡è£½ã€‚
        # æˆ‘å€‘æ”¹ç”¨ hstack, vstack ç›´æ¥æ‹¼æ¥ç¨€ç–å€å¡Šã€‚
        # çŸ©é™£çµæ§‹ A = [   0,    R ]
        #             [ R.T,    0 ]
        # ==========================================
        
        from scipy.sparse import csr_matrix, hstack, vstack
        
        R = self.UserItemNet.tocsr()
        RT = R.T
        
        # 1. å»ºç«‹å·¦ä¸Šè§’å’Œå³ä¸‹è§’çš„ 0 çŸ©é™£ (ä½¿ç”¨ CSR æ ¼å¼æ¥µçœç©ºé–“)
        # æ³¨æ„ï¼šä¸éœ€è¦çœŸçš„åˆ†é…è¨˜æ†¶é«”çµ¦ 0ï¼ŒCSR æ ¼å¼åªæœƒå­˜é 0 çš„å€¼
        zero_user = csr_matrix((n_users, n_users), dtype=np.float32)
        zero_item = csr_matrix((m_items, m_items), dtype=np.float32)
        
        # 2. æ‹¼æ¥çŸ©é™£
        # ä¸ŠåŠéƒ¨: [0, R]
        upper = hstack([zero_user, R])
        # ä¸‹åŠéƒ¨: [R.T, 0]
        lower = hstack([RT, zero_item])
        
        # æ•´å€‹ A
        adj_mat = vstack([upper, lower])
        adj_mat = adj_mat.tocsr()

        # ==========================================
        # ä»¥ä¸‹æ˜¯æ¨™æº–çš„ Normalization (D^-1/2 * A * D^-1/2)
        # ==========================================
        
        rowsum = np.array(adj_mat.sum(axis=1))
        
        # é˜²æ­¢é™¤ä»¥ 0 (åŠ ä¸Š 1e-7 æˆ–ç›´æ¥è¨­ 0)
        d_inv = np.power(rowsum, -0.5).flatten()
        d_inv[np.isinf(d_inv)] = 0.
        d_mat = sp.diags(d_inv)
        
        norm_adj = d_mat.dot(adj_mat).dot(d_mat)
        norm_adj = norm_adj.tocsr()
        
        self.Graph = self._convert_sp_mat_to_sp_tensor(norm_adj)
        self.Graph = self.Graph.coalesce().to(device)
        
        cprint("âœ… Adjacency matrix generated (Memory Optimized)!")
        return self.Graph