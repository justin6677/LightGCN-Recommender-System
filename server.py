from fastapi import FastAPI, HTTPException
import torch
import uvicorn
from contextlib import asynccontextmanager

# å¼•å…¥ä½ çš„æ¨¡çµ„
from src.model import LightGCN
from src.dataset import Loader

# å…¨åŸŸè®Šæ•¸ (ç”¨ä¾†æ”¾è¼‰å…¥å¥½çš„æ¨¡å‹ï¼Œé¿å…æ¯æ¬¡è«‹æ±‚éƒ½é‡æ–°è¼‰å…¥)
model_cache = {}

# è¨­å®šåƒæ•¸ (å¿…é ˆè·Ÿè¨“ç·´æ™‚ä¸€æ¨£)
config = {
    'latent_dim_rec': 64,
    'lightGCN_n_layers': 3,
    'A_n_fold': 100,
    'A_split': False,
    'bpr_batch_size': 2048,
    'dropout': False,
    'keep_prob': 0.6,
    'test_u_batch_size': 100,
    'multicore': 0,
    'lr': 0.001,
    'decay': 1e-4,
    'pretrain': 0
}

# === 1. å®šç¾©ä¼ºæœå™¨å•Ÿå‹•æ™‚çš„è¡Œç‚º (Lifespan) ===
# é€™æ®µç¨‹å¼ç¢¼ä¿è­‰æ¨¡å‹åªæœƒåœ¨ä¼ºæœå™¨å•Ÿå‹•æ™‚è¼‰å…¥ä¸€æ¬¡ (çœæ™‚é–“ï¼)
@asynccontextmanager
async def lifespan(app: FastAPI):
    print("ğŸš€ æ­£åœ¨å•Ÿå‹•æ¨è–¦ç³»çµ± API...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"é‹è¡Œè£ç½®: {device}")

    # 1. è¼‰å…¥è³‡æ–™ (å»ºç«‹ Graph)
    print("Loading Data...")
    # æ³¨æ„ï¼šé€™è£¡ä½¿ç”¨ max_users ä¾†åŠ å¿«å•Ÿå‹•é€Ÿåº¦ï¼Œæ­£å¼ç’°å¢ƒè«‹æ‹¿æ‰
    dataset = Loader(config, "./data/gowalla", max_users=1000, max_items=5000)
    
    # 2. åˆå§‹åŒ–æ¨¡å‹
    model = LightGCN(config, dataset).to(device)
    
    # 3. è¼‰å…¥æ¬Šé‡
    try:
        model.load_state_dict(torch.load("lightgcn_model.pth", map_location=device))
        print("âœ… æ¨¡å‹æ¬Šé‡è¼‰å…¥æˆåŠŸï¼")
        model.eval() # åˆ‡æ›åˆ°æ¨è«–æ¨¡å¼
    except FileNotFoundError:
        print("âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° lightgcn_model.pthï¼Œè«‹å…ˆåŸ·è¡Œ main.py è¨“ç·´æ¨¡å‹ï¼")
    
    # æŠŠæ¨¡å‹å­˜åˆ°å¿«å–ä¸­
    model_cache['model'] = model
    model_cache['device'] = device
    
    yield # é€™è£¡ä»£è¡¨ä¼ºæœå™¨é–‹å§‹é‹ä½œ
    
    print("ğŸ›‘ ä¼ºæœå™¨æ­£åœ¨é—œé–‰...")
    model_cache.clear()

# === 2. å»ºç«‹ FastAPI APP ===
app = FastAPI(title="LightGCN Recommender API", lifespan=lifespan)

# === 3. å®šç¾© API è·¯å¾‘ (Endpoint) ===

@app.get("/")
def read_root():
    return {"message": "æ­¡è¿ä¾†åˆ° LightGCN æ¨è–¦ç³»çµ± APIï¼è«‹è¨ªå• /docs æŸ¥çœ‹ä½¿ç”¨èªªæ˜ã€‚"}

@app.get("/recommend/{user_id}")
def recommend(user_id: int, k: int = 5):
    """
    è¼¸å…¥ User IDï¼Œå›å‚³ Top-K æ¨è–¦åˆ—è¡¨
    ä¾‹å¦‚: /recommend/10?k=5
    """
    model = model_cache.get('model')
    device = model_cache.get('device')
    
    if model is None:
        raise HTTPException(status_code=500, detail="æ¨¡å‹æœªè¼‰å…¥")

    # æª¢æŸ¥ User ID æ˜¯å¦åœ¨ç¯„åœå…§
    if user_id >= model.num_users:
        raise HTTPException(status_code=404, detail=f"User ID {user_id} ä¸å­˜åœ¨ (è¶…å‡ºç¯„åœ)")

    try:
        with torch.no_grad():
            user_tensor = torch.tensor([user_id], dtype=torch.long).to(device)
            ratings = model.getUsersRating(user_tensor)
            
            # å– Top-K
            _, topk_indices = torch.topk(ratings, k=k, dim=1)
            recs = topk_indices.cpu().numpy().flatten().tolist()
            
        return {
            "user_id": user_id,
            "top_k": k,
            "recommendations": recs
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# === 4. å¦‚æœç›´æ¥åŸ·è¡Œæ­¤æª”æ¡ˆï¼Œå‰‡å•Ÿå‹•ä¼ºæœå™¨ ===
if __name__ == "__main__":
    uvicorn.run("server:app", host="0.0.0.0", port=8000, reload=True)